{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nmduy/anaconda3/envs/graph/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.metrics.ranking module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from myfunctions import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from deep_pytorch import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== TRAINING SAMPLES =====\n",
      "Total Sample: 18316\n",
      "Sarcastic: 8726 (47.64%)\n",
      "Not Sarcastic: 9590 (52.36%)\n",
      "===== VALIDATING SAMPLES =====\n",
      "Total Sample: 4579\n",
      "Sarcastic: 2181 (47.63%)\n",
      "Not Sarcastic: 2398 (52.37%)\n",
      "===== TESTING SAMPLES =====\n",
      "Total Sample: 5724\n",
      "Sarcastic: 2727 (47.64%)\n",
      "Not Sarcastic: 2997 (52.36%)\n"
     ]
    }
   ],
   "source": [
    "data_full = pd.read_json('fake_news.json', lines=True)\n",
    "data_full = data_full.drop(columns=['article_link']) # remove link column\n",
    "df_train_f, df_test = split_dataframe(data_full, test_size=0.25, seed=1509)\n",
    "df_train, df_validate = split_dataframe(df_train_f, test_size=0.2, seed=1309)\n",
    "\n",
    "# Proportion of each subsets\n",
    "list_label = df_train['is_sarcastic'].tolist()\n",
    "numb_total = len(list_label)\n",
    "numb_sarcastic = np.sum(np.asarray(list_label))\n",
    "numb_not_sarcastic = numb_total - numb_sarcastic\n",
    "print(f'===== TRAINING SAMPLES =====\\nTotal Sample: {numb_total}\\nSarcastic: {numb_sarcastic} ({np.round(numb_sarcastic/numb_total*100,2)}%)\\nNot Sarcastic: {numb_not_sarcastic} ({np.round(numb_not_sarcastic/numb_total*100,2)}%)')\n",
    "\n",
    "list_label = df_validate['is_sarcastic'].tolist()\n",
    "numb_total = len(list_label)\n",
    "numb_sarcastic = np.sum(np.asarray(list_label))\n",
    "numb_not_sarcastic = numb_total - numb_sarcastic\n",
    "print(f'===== VALIDATING SAMPLES =====\\nTotal Sample: {numb_total}\\nSarcastic: {numb_sarcastic} ({np.round(numb_sarcastic/numb_total*100,2)}%)\\nNot Sarcastic: {numb_not_sarcastic} ({np.round(numb_not_sarcastic/numb_total*100,2)}%)')\n",
    "\n",
    "list_label = df_test['is_sarcastic'].tolist()\n",
    "numb_total = len(list_label)\n",
    "numb_sarcastic = np.sum(np.asarray(list_label))\n",
    "numb_not_sarcastic = numb_total - numb_sarcastic\n",
    "print(f'===== TESTING SAMPLES =====\\nTotal Sample: {numb_total}\\nSarcastic: {numb_sarcastic} ({np.round(numb_sarcastic/numb_total*100,2)}%)\\nNot Sarcastic: {numb_not_sarcastic} ({np.round(numb_not_sarcastic/numb_total*100,2)}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = df_train\n",
    "data_train['headline_s1'] = data_train.headline.apply(lambda row: remove_symbol(row))\n",
    "data_train['headline_s2'] = data_train.headline_s1.apply(lambda row: lemmatize_word(row, 'v'))\n",
    "data_train['headline_s2'] = data_train.headline_s2.apply(lambda row: lemmatize_word(row, 'n'))\n",
    "data_train = data_train.drop(columns=['headline', 'headline_s1'])\n",
    "\n",
    "data_val = df_validate\n",
    "data_val['headline_s1'] = data_val.headline.apply(lambda row: remove_symbol(row))\n",
    "data_val['headline_s2'] = data_val.headline_s1.apply(lambda row: lemmatize_word(row, 'v'))\n",
    "data_val['headline_s2'] = data_val.headline_s2.apply(lambda row: lemmatize_word(row, 'n'))\n",
    "data_val = data_val.drop(columns=['headline', 'headline_s1'])\n",
    "\n",
    "data_train_rmsw = data_train.copy()\n",
    "data_train_rmsw['headline_s3'] = data_train_rmsw.headline_s2.apply(lambda row: remove_stop_words(row))\n",
    "data_train_rmsw = data_train_rmsw.drop(columns=['headline_s2'])\n",
    "\n",
    "data_val_rmsw = data_val.copy()\n",
    "data_val_rmsw['headline_s3'] = data_val_rmsw.headline_s2.apply(lambda row: remove_stop_words(row))\n",
    "data_val_rmsw = data_val_rmsw.drop(columns=['headline_s2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW Entire Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing FT_0\n",
      "Processing FT_1000\n",
      "Processing FT_2000\n",
      "Processing FT_3000\n",
      "Processing FT_4000\n",
      "Processing FT_5000\n",
      "Processing FT_6000\n",
      "Processing FT_7000\n",
      "Processing FT_8000\n",
      "Processing FT_9000\n",
      "Processing FT_10000\n",
      "Processing FT_11000\n",
      "Processing FT_12000\n",
      "Processing FT_13000\n",
      "Processing FT_14000\n",
      "Processing FT_15000\n",
      "Processing FT_16000\n",
      "Processing FT_17000\n",
      "Processing FT_18000\n",
      "Processing FT_0\n",
      "Processing FT_1000\n",
      "Processing FT_2000\n",
      "Processing FT_3000\n",
      "Processing FT_4000\n",
      "Processing FT_5000\n",
      "Processing FT_6000\n",
      "Processing FT_7000\n",
      "Processing FT_8000\n",
      "Processing FT_9000\n",
      "Processing FT_10000\n",
      "Processing FT_11000\n",
      "Processing FT_12000\n",
      "Processing FT_13000\n",
      "Processing FT_14000\n",
      "Processing FT_15000\n",
      "Processing FT_16000\n",
      "Processing FT_17000\n",
      "Processing FT_18000\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "all_string = data_train.headline_s2.tolist()\n",
    "all_string_in_one = ' '.join(all_string)\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit([all_string_in_one])\n",
    "# encode document\n",
    "vector = vectorizer.transform(data_train.headline_s2.tolist())\n",
    "# summarize encoded vector\n",
    "vector = vector.toarray()\n",
    "data_train = data_train.drop(columns=['headline_s2'])\n",
    "\n",
    "for idx in range(vector.shape[1]):\n",
    "    if idx % 1000 == 0:\n",
    "        print(f'Processing FT_{idx}')\n",
    "    data_train[f\"ft_{idx}\"] = vector[:,idx]\n",
    "    \n",
    "# encode document\n",
    "vector = vectorizer.transform(data_val.headline_s2.tolist())\n",
    "# summarize encoded vector\n",
    "vector = vector.toarray()\n",
    "data_val = data_val.drop(columns=['headline_s2'])\n",
    "\n",
    "for idx in range(vector.shape[1]):\n",
    "    if idx % 1000 == 0:\n",
    "        print(f'Processing FT_{idx}')\n",
    "    data_val[f\"ft_{idx}\"] = vector[:,idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train.drop(columns=['is_sarcastic'])\n",
    "y_train = data_train['is_sarcastic']\n",
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "\n",
    "X_validate = data_val.drop(columns=['is_sarcastic'])\n",
    "y_validate = data_val['is_sarcastic']\n",
    "X_validate = np.asarray(X_validate)\n",
    "y_validate = np.asarray(y_validate)\n",
    "\n",
    "feature_names = list(data_val.drop(columns=['is_sarcastic']).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6641188032321468,\n",
       " 'precision': 0.7620211898940505,\n",
       " 'recall': 0.4287024300779459,\n",
       " 'f1': 0.5487089201877935,\n",
       " 'tp': 935,\n",
       " 'tn': 2106,\n",
       " 'fp': 292,\n",
       " 'fn': 1246,\n",
       " 'auc': 0.6559682740354851}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nb = GaussianNB()\n",
    "model_nb.fit(X_train, y_train)\n",
    "\n",
    "validate_pred = model_nb.predict_proba(X_validate)\n",
    "validate_pred = validate_pred[:,1]\n",
    "metrics = calculate_metric(y_validate, validate_pred)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hypara = {\n",
    "    'random_state': 1509,\n",
    "    'max_depth': 20,\n",
    "    'n_estimators': 300,\n",
    "    'min_samples_split': 7,\n",
    "    'min_samples_leaf': 2,\n",
    "}\n",
    "model_rf = RandomForestClassifier(**hypara)\n",
    "model_rf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7532212273422144,\n",
       " 'precision': 0.8631651693158259,\n",
       " 'recall': 0.572673085740486,\n",
       " 'f1': 0.6885336273428886,\n",
       " 'tp': 1249,\n",
       " 'tn': 2200,\n",
       " 'fp': 198,\n",
       " 'fn': 932,\n",
       " 'auc': 0.8724176573860457}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_pred = model_rf.predict_proba(X_validate)\n",
    "validate_pred = validate_pred[:,1]\n",
    "metrics = calculate_metric(y_validate, validate_pred)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM (Too Long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypara = {\n",
    "    'C': 100,\n",
    "    'random_state': 1509,\n",
    "    'max_iter':10000,\n",
    "    'tol' : 1e-5\n",
    "}\n",
    "model_svm = SVC(**hypara)\n",
    "model_svm.fit(X_train, y_train);\n",
    "\n",
    "validate_pred = model_svm.predict(X_validate)\n",
    "metrics = calculate_metric(y_validate, validate_pred)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5383271456649924,\n",
       " 'precision': 0.958904109589041,\n",
       " 'recall': 0.03209536909674461,\n",
       " 'f1': 0.062111801242236024,\n",
       " 'tp': 70,\n",
       " 'tn': 2395,\n",
       " 'fp': 3,\n",
       " 'fn': 2111,\n",
       " 'auc': 0.7645183457558051}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypara = {\n",
    "    'n_neighbors': 200,\n",
    "    'weights': 'distance'\n",
    "}\n",
    "model_knn = KNeighborsClassifier(**hypara)\n",
    "model_knn.fit(X_train, y_train);\n",
    "\n",
    "validate_pred = model_knn.predict_proba(X_validate)\n",
    "validate_pred = validate_pred[:,1]\n",
    "metrics = calculate_metric(y_validate, validate_pred)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypara = {\n",
    "    'penalty': 'l1',\n",
    "    'C': 5,\n",
    "    'max_iter': 5000,\n",
    "    'tol': 1e-5,\n",
    "    'solver': 'saga'\n",
    "}\n",
    "\n",
    "model_lr = LogisticRegression(**hypara)\n",
    "model_lr.fit(X_train, y_train);\n",
    "validate_pred = model_lr.predict_proba(X_validate)\n",
    "validate_pred = validate_pred[:,1]\n",
    "metrics = calculate_metric(y_validate, validate_pred)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
